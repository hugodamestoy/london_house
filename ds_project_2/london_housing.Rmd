---
title: 'Session 10: Data Science Capstone Project'
author: "Tolga Tezcan"
date: "`r Sys.Date()`"
output: 
    html_document:
      number_sections: true
      highlight: haddock
      theme: spacelab
      toc: yes
      toc_depth: 2
      toc_float:
        collapsed: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<style>
div.navy1 { background-color:#686868; border-radius: 5px; padding: 20px; border-style: groove; color: #ffffff;}

</style>


```{r, load_libraries, include = FALSE}

if(!is.element("tidyverse", installed.packages()[,1]))
{  install.packages("tidyverse")}

if(!is.element("Hmisc", installed.packages()[,1]))
{  install.packages("Hmisc")} #package for data summary using `describe`

if(!is.element("ggplot2", installed.packages()[,1]))
{  install.packages("ggplot2")} #package for plots
if(!is.element("ggthemes", installed.packages()[,1]))
{  install.packages("ggthemes")} #package to make fancier ggplots

if(!is.element("janitor", installed.packages()[,1]))
{ install.packages("janitor")} #package to visualize results of machine learning tools
if(!is.element("rpart.plot", installed.packages()[,1]))
{  install.packages("rpart.plot")} #package to visualize trees

library(rpart.plot)
library(caret)
library(tidyverse) # the usual stuff: dplyr, readr, and other goodies
library(lubridate)
library(janitor) # clean_names()
library(Hmisc)
```



# Load data


```{r read-investigate}
#read in the data

london_house_prices_2019_training<-read.csv("training_data_assignment_with_prices.csv")
london_house_prices_2019_out_of_sample<-read.csv("test_data_assignment.csv")



#fix data types in both data sets

#fix dates
london_house_prices_2019_training <- london_house_prices_2019_training %>% mutate(date=as.Date(date))
london_house_prices_2019_out_of_sample<-london_house_prices_2019_out_of_sample %>% mutate(date=as.Date(date))
#change characters to factors
london_house_prices_2019_training <- london_house_prices_2019_training %>% mutate_if(is.character,as.factor)
london_house_prices_2019_out_of_sample<-london_house_prices_2019_out_of_sample %>% mutate_if(is.character,as.factor)



#make sure out of sample data and training data has the same levels for factors 
a<-union(levels(london_house_prices_2019_training$postcode_short),levels(london_house_prices_2019_out_of_sample$postcode_short))
london_house_prices_2019_out_of_sample$postcode_short <- factor(london_house_prices_2019_out_of_sample$postcode_short, levels = a)
london_house_prices_2019_training$postcode_short <- factor(london_house_prices_2019_training$postcode_short, levels = a)

a<-union(levels(london_house_prices_2019_training$district),levels(london_house_prices_2019_out_of_sample$district))
london_house_prices_2019_out_of_sample$district <- factor(london_house_prices_2019_out_of_sample$district, levels = a)
london_house_prices_2019_training$district <- factor(london_house_prices_2019_training$district, levels = a)

a<-union(levels(london_house_prices_2019_training$nearest_station),levels(london_house_prices_2019_out_of_sample$nearest_station))
london_house_prices_2019_out_of_sample$nearest_station <- factor(london_house_prices_2019_out_of_sample$nearest_station, levels = a)
london_house_prices_2019_training$nearest_station <- factor(london_house_prices_2019_training$nearest_station, levels = a)

#take a quick look at what's in the data
#str(london_house_prices_2019_training)
#str(london_house_prices_2019_out_of_sample)



```


```{r split the price data to training and testing}
#let's do the initial split
library(rsample)
set.seed(1)
train_test_split <- initial_split(london_house_prices_2019_training, prop = 0.75) #training set contains 75% of the data
# Create the training dataset

train_data <- training(train_test_split)
test_data <- testing(train_test_split)



```


# Visualize data 

Visualize and examine the data

```{r visualize}
#check range of dates
#describe(london_house_prices_2019_training$date)

#price repartition
ggplot(london_house_prices_2019_training, aes(x=price))+ geom_histogram(bins = 60)+xlim(0,3000000)+theme_bw()+ labs(title= "Price distribution")+theme_get()



#price and distance to station 
#ggplot(london_house_prices_2019_training, aes(x=distance_to_station, y=price))+ geom_point() +geom_smooth(method = "lm", formula = y ~ x)

#price and floor area
ggplot(london_house_prices_2019_training, aes(x=total_floor_area, y=price))+ geom_point() +geom_smooth(method = "lm", formula = y ~ x)+theme_get()+  labs(title = "Impact of floor area on prices")

#price and number habitable rooms 
#ggplot(london_house_prices_2019_training, aes(x=number_habitable_rooms, y=price))+ geom_point() +geom_smooth(method = "lm", formula = y ~ x)

#price and property type
#ggplot(london_house_prices_2019_training, aes(x=property_type, y=price))+ geom_boxplot()
#ggplot(london_house_prices_2019_training, aes(x=property_type, y=price))+ geom_boxplot()+ ylim(0,2000000)


#price and district 
list_of_district<- c("Lambeth", "Croydon", "Kensington and Chelsea", "Westminster")
london_house_prices_2019_training_2 <-london_house_prices_2019_training %>% filter(london_house_prices_2019_training$district %in% list_of_district)
ggplot(london_house_prices_2019_training_2, aes(x=district, y=price))+ geom_boxplot()+ theme_get()+ labs(title = "Impact of district on prices")
ggplot(london_house_prices_2019_training_2, aes(x=total_floor_area, y=price, color=district))+ geom_point() +geom_smooth(method = "lm", formula = y ~ x, se=FALSE)+ labs(title = "Impact of total_floor_area*district on price")+theme_get()


#price and london_zone
#convert zone into a factor variable
#london_house_prices_2019_training_3 <- london_house_prices_2019_training
#london_house_prices_2019_training_3$london_zone<- as.factor(london_house_prices_2019_training_3$london_zone)
#ggplot(london_house_prices_2019_training_3, aes(x=london_zone, y=price))+ geom_boxplot()
#ggplot(london_house_prices_2019_training_3, aes(x=london_zone, y=price))+ geom_boxplot()+ ylim(0,2000000)+theme_get()+labs(title = "London zone as a factor")


```

Estimate a correlation table between prices and other continuous variables.

```{r, correlation table, warning=FALSE, message=FALSE}

# produce a correlation table using GGally::ggcor()
# this takes a while to plot

library("GGally")
london_house_prices_2019_training %>% 
  select(-ID) %>% #keep Y variable last
  ggcorr(method = c("pairwise", "pearson"), layout.exp = 2,label_round=2, label = TRUE,label_size = 2,hjust = 1,nbreaks = 5,size = 2,angle = -20)+labs(title = "Correlation table")

```


# Fit a linear regression model


```{r,warning=FALSE,  message=FALSE }
#Here we will start by define the trainControl that we will use for each method, as we need to have the same control parameter in order to stack them

#number of folds in cross validation
CVfolds <- 10

#Define folds
set.seed(1)
  #create five folds with no repeats
indexPreds <- createMultiFolds(train_data$price, CVfolds,times = 1) 
#Define traincontrol using folds
ctrl <- trainControl(method = "cv",  number = CVfolds, returnResamp = "final", savePredictions = "final", index = indexPreds,sampling = NULL)


```

```{r LR model}

# We are going to use a forward selection model


# We are going to train the model and report the results using k-fold cross validation
# We are going to use a forward selection model, starting by adding the variable with the highest correlation and etc

#price ~ total_floor_area,
    #train_data,
   #method = "lm",
    #trControl = ctrl
   #)
#model1_lm<-train(
   # price ~ total_floor_area + co2_emissions_current,
    #train_data,
   #method = "lm",
    #trControl = ctrl
   ##)
# When we add the Co2 current emission R squared is not improving so I don't believe we should use this variable in our prediction
#model1_lm<-train(
    #price ~ total_floor_area + co2_emissions_potential,
   # train_data,
   #method = "lm",
   # trControl = ctrl
   #)
#I continue to do so

#model1_lm<-train(
    #price ~ total_floor_area + co2_emissions_potential+number_habitable_rooms+average_income+ num_tube_lines+ longitude + num_rail_lines + distance_to_station ,
    #train_data,
   #method = "lm",
   # trControl = ctrl
  # )

# Now we will start using factorial vraible
# not showing every model I try but the idea, is I look at the p-value and the adj R squared if the p-value is under 0,05 and the adj Rsquared is increasig I keep this variable, adding new variables also decrease the explanatory powe of other so when one of my old variable reach a p_value > 0,05 I remove this vriable ex: distance_to_station , num_rail_lines. i will also try some interaction term


model1_lm<-train(
    price ~ total_floor_area + co2_emissions_potential+number_habitable_rooms+average_income+ num_tube_lines+ longitude   + property_type + london_zone + freehold_or_leasehold  +  total_floor_area*district + total_floor_area*property_type + london_zone*total_floor_area +london_zone*property_type,
    train_data ,
   method = "lm",
    trControl = ctrl
   )



# summary of the results
summary(model1_lm)

# We can predict the testing values

predictions <- predict(model1_lm,test_data)

lr_results<-data.frame(  RMSE = RMSE(predictions, test_data$price), 
                            Rsquare = R2(predictions, test_data$price))

                            
lr_results                         

#We can predict prices for out of sample data the same way
predictions_oos <- predict(model1_lm,london_house_prices_2019_out_of_sample)

#RMSE and Rsquare are similar in both the testing and training, there is no overfitting and this model should probably be able to predict quite accurately the value of houses in the out of sample data.

# we can check variable importance as well
#importance <- varImp(model1_lm, scale=TRUE)
#plot(importance, 10)

```


# Fit a tree model


```{r tree model}

#model2_tree <- train(
  #price ~  total_floor_area + +latitude+ longitude,
 # train_data,
  #method = "rpart",
  #trControl = ctrl,
  #tuneLength=100
   # )

## did exactly the same as with regression, and try to optimise adj R squared by yuning cp and adding right variables

model2_tree <- train(
  price ~  total_floor_area + co2_emissions_potential+number_habitable_rooms+average_income+ num_tube_lines+ longitude + property_type + london_zone + freehold_or_leasehold   + total_floor_area*district + london_zone*total_floor_area+total_floor_area*property_type,
  train_data,
  method = "rpart",
  trControl = ctrl,
  tuneLength=200
    )

#plot(model2_tree)

##tune cp decison treee have the tendency to overfit, complexity parameter 

#visualising results
model2_tree$results
#rpart.plot(model2_tree$finalModel)

#Variable importance
#importance <- varImp(model2_tree, scale=TRUE)
#plot(importance, 10)


predictions <- predict(model2_tree,test_data)

lr_results<-data.frame(  RMSE = RMSE(predictions, test_data$price), 
                            Rsquare = R2(predictions, test_data$price))

                            
lr_results             

```


# Random Forrest

```{r,warning=FALSE,  message=FALSE }
##Random Forrest 

#gridRF <- data.frame(
  #.mtry = 2,
  #.splitrule = "variance",
  #.min.node.size = 6
#)


#model4_rf <- train(
 #price ~ total_floor_area  ,
  #train_data,
  #method = "ranger",
 #trControl = ctrl,
  #tuneGrid = gridRF,
  #importance = 'impurity')


## same thing adding vaiables and switching values for the gridRF and adding variables


gridRF <- data.frame(
  .mtry = 3,
  .splitrule = "variance",
  .min.node.size = 5
)


model4_rf <- train(
 price ~ total_floor_area + latitude + longitude + average_income +london_zone +property_type ,
  train_data,
  method = "ranger",
 trControl = ctrl,
  tuneGrid = gridRF,
  importance = 'impurity')

#summary(model4_rf)
print(model4_rf)

predictions <- predict(model4_rf,test_data)

lr_results<-data.frame(  RMSE = RMSE(predictions, test_data$price), 
                            Rsquare = R2(predictions, test_data$price))

                            
lr_results  

#importance <- varImp(model4_rf, scale=TRUE)
#plot(importance)
```


# KNN

```{r,warning=FALSE,  message=FALSE }



#knnGrid <-  expand.grid(k= seq(1,20 , by = 1)) 


#model3_knn <- train(price ~ total_floor_area + co2_emissions_potential+number_habitable_rooms+average_income+ num_tube_lines+ longitude   + property_type + london_zone + freehold_or_leasehold + london_zone*total_floor_area,
                # data=train_data, 
                 #preProcess = c("center", "scale"), 
                 #method="knn", 
                 #metric="RMSE", 
                 #trControl = ctrl,
                 #tuneGrid = knnGrid)
# display results
#print(model3_knn)

#plot(model3_knn)


#try a lot of value and 5 gave me the best RMSE withthe new variable total_floor_area*district, but it's taking a long time to compute so I only show with k=5
knnGrid <-  expand.grid(k= 5)

model3_knn <- train(price ~ total_floor_area + co2_emissions_potential+number_habitable_rooms+average_income+ num_tube_lines+ longitude   + property_type + london_zone + freehold_or_leasehold + london_zone*total_floor_area+ total_floor_area*district,
                 data=train_data, 
                 preProcess = c("center", "scale"), 
                 method="knn", 
                 metric="RMSE", 
                 trControl = ctrl,
                 tuneGrid = knnGrid)

print(model3_knn)


predictions <- predict(model3_knn,test_data)

lr_results<-data.frame(  RMSE = RMSE(predictions, test_data$price), 
                            Rsquare = R2(predictions, test_data$price))

                            
lr_results  

#importance <- varImp(model3_knn, scale=TRUE)
#plot(importance, 10)

```


# Stacking

```{r stacking}
#we can now use stacking with the list of models
library(caretEnsemble)
multimodel <- list(lm = model1_lm, rpart = model2_tree, knn = model3_knn, ranger= model4_rf)
class(multimodel) <- "caretList"
  model_list <- caretStack(multimodel,
    trControl=ctrl,method="lm",
    metric = "RMSE")

  summary(model_list)
  

  
# tree is not significant, I do not want to use it
  



multimodel <- list(lm = model1_lm, knn = model3_knn, ranger= model4_rf)
class(multimodel) <- "caretList"
 model_list <- caretStack(multimodel,
    trControl=ctrl,method="lm",
    metric = "RMSE")

  summary(model_list)

  
#we can visualize the differences in performance of each algorithm for each fold 
  #modelCor(resamples(multimodel))
  #dotplot(resamples(multimodel), metric = "Rsquared") #you can set metric=MAE, RMSE, or Rsquared 
   #xyplot(resamples(multimodel), metric = "Rsquared")
    #splom(resamples(multimodel), metric = "Rsquared")
  
  predictions <- predict(model_list,test_data)

lr_results<-data.frame(  RMSE = RMSE(predictions, test_data$price), 
                            Rsquare = R2(predictions, test_data$price))

                            
lr_results  
  
```


# Pick investments

In this section I use my best algorithms to choose the investment

```{r,warning=FALSE,  message=FALSE }


numchoose=200

oos<-london_house_prices_2019_out_of_sample

#predict the value of houses
oos$predict <- predict(model_list,oos)
#Choose the ones you want to invest here
oos<- oos%>%mutate(profitMargin=(predict-asking_price)/asking_price)%>%arrange(-profitMargin)
#Make sure you choose exactly 200 of them
oos$buy=0
oos[1:numchoose,]$buy=1

oos<-oos %>%mutate(profit=(predict-asking_price)/asking_price)%>%mutate(actualProfit=buy*profit)

#if we invest in everything
#mean(oos$profit)

#just invest in those we chose
#sum(oos$actualProfit)/numchoose

#output your choices. Change the name of the file to your "lastname_firstname.csv"
write.csv(oos,"damestoy_hugo.csv")

```
